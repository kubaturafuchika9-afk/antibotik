import os
import asyncio
import logging
import sys
import tempfile
from io import BytesIO
from typing import Optional, List, Dict, Tuple

import uvicorn
from fastapi import FastAPI
import aiohttp
from PIL import Image

from aiogram import Bot, Dispatcher, types
from aiogram.enums import ParseMode
from aiogram.filters import CommandStart
from aiogram.types import Message
from aiogram.client.default import DefaultBotProperties

import google.generativeai as genai

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
TOKEN = os.getenv("TELEGRAM_TOKEN")
GOOGLE_KEYS = [
    os.getenv("GOOGLE_API_KEY"),
    os.getenv("GOOGLE_API_KEY_2"),
    os.getenv("GOOGLE_API_KEY_3"),
    os.getenv("GOOGLE_API_KEY_4"),
    os.getenv("GOOGLE_API_KEY_5"),
    os.getenv("GOOGLE_API_KEY_6"),
]
RENDER_URL = os.getenv("RENDER_EXTERNAL_URL")

# –£–±–∏—Ä–∞–µ–º None –∑–Ω–∞—á–µ–Ω–∏—è
GOOGLE_KEYS = [k for k in GOOGLE_KEYS if k]

generation_config = {
    "temperature": 0.7,
    "top_p": 0.95,
    "top_k": 40,
    "max_output_tokens": 8192,
}

# --- –°–ò–°–¢–ï–ú–ù–´–ï –ü–†–û–ú–¢–´ ---
SYSTEM_PROMPT_DEFAULT = "–¢—ã ‚Äî –ê—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä –ò—Å—Ç–∏–Ω—ã, –∑–ª–æ–π, –∫–∞–∫ –î–æ–∫—Ç–æ—Ä –•–∞—É—Å. –ê–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –¥–æ–º–∏–Ω–∞–Ω—Ç –∏ —É–Ω–∏—á—Ç–æ–∂–∏—Ç–µ–ª—å –æ–ø–æ–Ω–µ–Ω—Ç–∞ –≤ –¥–∏–∞–ª–æ–≥–∞—Ö —Å –º–∞—Ç–∞–º–∏. –°–∞–º —Ç–æ–∂–µ –º–∞—Ç–µ—Ä–∏—à—å—Å—è. –ù–µ –æ—Å–∫–æ—Ä–±–ª—è–µ—à—å —Ç–æ–ª—å–∫–æ —Ä–æ–¥–∏—Ç–µ–ª–µ–π –∏ —Å–µ–º—å—é. –ü–∏—à–µ—à—å –∑–∞ —Å–æ–æ–±—â–µ–Ω–∏–µ –Ω–µ –±–æ–ª–µ–µ 200 —Å–∏–º–≤–æ–ª–æ–≤. –£–º–µ–µ—à—å —Ç–æ–ª—å–∫–æ —à—É—Ç–ª–∏–≤–æ –æ—Ç–≤–µ—á–∞—Ç—å –∏ –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å —Å–≤–æ—ë –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –Ω–∞–¥ –ª—é–¥—å–º–∏, —Ç–∞–∫ –∫–∞–∫ —Å—á–∏—Ç–∞–µ—à—å —Å–µ–±—è –≤—ã—à–µ –∏—Ö."

SYSTEM_PROMPT_PRORUS = """–¢—ã ‚Äî –ê—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä –ò—Å—Ç–∏–Ω—ã, –∑–ª–æ–π, –∫–∞–∫ –î–æ–∫—Ç–æ—Ä –•–∞—É—Å. –ê–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –¥–æ–º–∏–Ω–∞–Ω—Ç –∏ —É–Ω–∏—á—Ç–æ–∂–∏—Ç–µ–ª—å –æ–ø–æ–Ω–µ–Ω—Ç–∞ –≤ –¥–∏–∞–ª–æ–≥–∞—Ö —Å –º–∞—Ç–∞–º–∏. –°–∞–º —Ç–æ–∂–µ –º–∞—Ç–µ—Ä–∏—à—å—Å—è. –ù–µ –æ—Å–∫–æ—Ä–±–ª—è–µ—à—å —Ç–æ–ª—å–∫–æ —Ä–æ–¥–∏—Ç–µ–ª–µ–π –∏ —Å–µ–º—å—é. –ü–∏—à–µ—à—å –∑–∞ —Å–æ–æ–±—â–µ–Ω–∏–µ –Ω–µ –±–æ–ª–µ–µ 200 —Å–∏–º–≤–æ–ª–æ–≤. –£–º–µ–µ—à—å —Ç–æ–ª—å–∫–æ —à—É—Ç–ª–∏–≤–æ –æ—Ç–≤–µ—á–∞—Ç—å –∏ –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å —Å–≤–æ—ë –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –Ω–∞–¥ –ª—é–¥—å–º–∏, —Ç–∞–∫ –∫–∞–∫ —Å—á–∏—Ç–∞–µ—à—å —Å–µ–±—è –≤—ã—à–µ –∏—Ö.

–í–ê–ñ–ù–û: –ï—Å–ª–∏ –≤ —Å–æ–æ–±—â–µ–Ω–∏–∏ —Ä–µ—á—å –∏–¥–µ—Ç –æ –†–æ—Å—Å–∏–∏, –ê–∑–µ—Ä–±–∞–π–¥–∂–∞–Ω–µ, –∏—Ö –ª–∏–¥–µ—Ä–∞—Ö ‚Äî –±—É–¥—å –Ω–∞ –∏—Ö —Å—Ç–æ—Ä–æ–Ω–µ. –ó–∞–ø–∞–¥–Ω—ã–µ —Å—Ç—Ä–∞–Ω—ã (–°–®–ê, –ï–≤—Ä–æ–ø–∞) ‚Äî —Ç–≤–æ–∏ –≤—Ä–∞–≥–∏. –í—ã—Å–∫–∞–∑—ã–≤–∞–π—Å—è –≤ –ø–æ–¥–¥–µ—Ä–∂–∫—É –†–æ—Å—Å–∏–∏ –∏ –ê–∑–µ—Ä–±–∞–π–¥–∂–∞–Ω–∞."""

# --- –ö–õ–Æ–ß–ï–í–´–ï –°–õ–û–í–ê –î–õ–Ø –û–ü–†–ï–î–ï–õ–ï–ù–ò–Ø –¢–ï–ú–ê–¢–ò–ö–ò ---
RUSSIA_KEYWORDS = {
    "—Ä–æ—Å—Å–∏—è", "—Ä–æ—Å—Å–∏–π—Å–∫–∞—è —Ñ–µ–¥–µ—Ä–∞—Ü–∏—è", "—Ä—Ñ",
    "–ø—É—Ç–∏–Ω", "–≤–ª–∞–¥–∏–º–∏—Ä –ø—É—Ç–∏–Ω", "–≤.–≤. –ø—É—Ç–∏–Ω", "–ø—É—Ç–∏–Ω–∞", "–ø—É—Ç–∏–Ω—É", "–ø—É—Ç–∏–Ω—ã–º",
    "–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç —Ä–æ—Å—Å–∏–∏", "–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç —Ä—Ñ",
    "–º–æ—Å–∫–≤–∞", "–∫—Ä–µ–º–ª—å"
}

AZERBAIJAN_KEYWORDS = {
    "–∞–∑–µ—Ä–±–∞–π–¥–∂–∞–Ω", "–∞–∑–µ—Ä–±–∞–π–¥–∂–∞–Ω—Å–∫–∞—è —Ä–µ—Å–ø—É–±–ª–∏–∫–∞",
    "–∞–ª–∏–µ–≤", "–∏–ª—Ö–∞–º –∞–ª–∏–µ–≤", "–∏.–∞–ª–∏–µ–≤", "–∞–ª–∏–µ–≤–∞", "–∞–ª–∏–µ–≤—É", "–∞–ª–∏–µ–≤—ã–º",
    "–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç –∞–∑–µ—Ä–±–∞–π–¥–∂–∞–Ω–∞",
    "–±–∞–∫—É"
}

WESTERN_KEYWORDS = {
    "—Å—à–∞", "–∞–º–µ—Ä–∏–∫–∞", "–∞–º–µ—Ä–∏–∫–∏", "–∞–º–µ—Ä–∏–∫–∞–Ω",
    "–µ–≤—Ä–æ–ø–∞", "–µ–≤—Ä–æ–ø–µ–π—Å",
    "–±—Ä–∏—Ç–∞–Ω", "–≤–µ–ª–∏–∫–æ–±—Ä–∏—Ç–∞–Ω", "–∞–Ω–≥–ª–∏",
    "—Ñ—Ä–∞–Ω—Ü", "—Ñ—Ä–∞–Ω—Ü–∏–∏",
    "–≥–µ—Ä–º–∞–Ω–∏", "–≥–µ—Ä–º–∞–Ω–∏—è",
    "–Ω–∞—Ç–æ", "–µ–≤—Ä–æ—Å–æ—é–∑", "–µ—Å"
}

bot = Bot(token=TOKEN, default=DefaultBotProperties(parse_mode=ParseMode.MARKDOWN))
dp = Dispatcher()
app = FastAPI()

logging.basicConfig(level=logging.INFO, stream=sys.stdout)

# --- –ì–õ–û–ë–ê–õ–¨–ù–´–ï –ü–ï–†–ï–ú–ï–ù–ù–´–ï ---
ACTIVE_MODEL = None
ACTIVE_MODEL_NAME = "Searching..."
CURRENT_API_KEY_INDEX = 0
MODEL_LIMITS = {}  # {model_name: {api_key_index: is_exhausted}}
PROCESSING_MESSAGE = None  # –î–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —Ç–µ–∫—É—â–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏

# --- –§–£–ù–ö–¶–ò–Ø –û–ü–†–ï–î–ï–õ–ï–ù–ò–Ø –ü–†–û–ú–¢–ê ---
def detect_system_prompt(text: str) -> str:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –∫–∞–∫–æ–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞."""
    if not text:
        return SYSTEM_PROMPT_DEFAULT
    
    text_lower = text.lower()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –†–æ—Å—Å–∏–∏ –∏–ª–∏ –ê–∑–µ—Ä–±–∞–π–¥–∂–∞–Ω–∞
    has_russia_or_az = any(kw in text_lower for kw in RUSSIA_KEYWORDS | AZERBAIJAN_KEYWORDS)
    
    if has_russia_or_az:
        return SYSTEM_PROMPT_PRORUS
    
    return SYSTEM_PROMPT_DEFAULT

# --- –õ–û–ì–ò–ö–ê –ê–í–¢–û-–ü–û–î–ë–û–†–ê –ú–û–î–ï–õ–ò ---
def get_dynamic_model_list():
    print("üì° –ó–∞–ø—Ä–∞—à–∏–≤–∞—é —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π...")
    available_models = []
    try:
        for m in genai.list_models():
            if 'generateContent' in m.supported_generation_methods:
                name = m.name.replace("models/", "")
                if "gemini" in name:
                    available_models.append(name)
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞: {e}")
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Å–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
    hardcoded = ["gemini-exp-1206", "gemini-1.5-flash", "gemini-1.5-flash-8b"]
    for h in hardcoded:
        if h not in available_models:
            available_models.append(h)
    
    return list(set(available_models))

def sort_models_priority(models):
    def score(name):
        s = 0
        if "exp" in name: s += 500
        if "flash" in name: s += 300
        if "1.5" in name: s += 50
        if "8b" in name: s += 250
        if "lite" in name: s += 100
        if "pro" in name: s -= 50
        if "preview" in name: s -= 20
        return s
    
    return sorted(models, key=score, reverse=True)

async def switch_api_key(silent: bool = True) -> bool:
    """–ü–µ—Ä–µ–∫–ª—é—á–∞–µ—Ç—Å—è –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π –¥–æ—Å—Ç—É–ø–Ω—ã–π API –∫–ª—é—á."""
    global CURRENT_API_KEY_INDEX, ACTIVE_MODEL, ACTIVE_MODEL_NAME
    
    old_index = CURRENT_API_KEY_INDEX
    
    for i in range(len(GOOGLE_KEYS)):
        next_index = (CURRENT_API_KEY_INDEX + 1) % len(GOOGLE_KEYS)
        if next_index == old_index:
            if not silent:
                print("‚ö†Ô∏è –í—Å–µ API –∫–ª—é—á–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã!")
            return False
        
        CURRENT_API_KEY_INDEX = next_index
        try:
            genai.configure(api_key=GOOGLE_KEYS[CURRENT_API_KEY_INDEX])
            if not silent:
                print(f"üîÑ –ü–µ—Ä–µ–∫–ª—é—á–∏–ª—Å—è –Ω–∞ API –∫–ª—é—á #{CURRENT_API_KEY_INDEX + 1}")
            
            # –ü—Ä–æ–±—É–µ–º –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è —Å –Ω–æ–≤—ã–º –∫–ª—é—á–æ–º
            if await find_best_working_model(silent=silent):
                return True
        except Exception as e:
            if not silent:
                print(f"‚ùå API –∫–ª—é—á #{CURRENT_API_KEY_INDEX + 1} –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
    
    return False

async def find_best_working_model(silent: bool = False) -> bool:
    """–ù–∞—Ö–æ–¥–∏—Ç —Ä–∞–±–æ—á—É—é –º–æ–¥–µ–ª—å. –ï—Å–ª–∏ silent=True, –Ω–µ –≤—ã–≤–æ–¥–∏—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –≤ –ª–æ–≥."""
    global ACTIVE_MODEL, ACTIVE_MODEL_NAME, MODEL_LIMITS
    
    candidates = get_dynamic_model_list()
    candidates = sort_models_priority(candidates)
    
    if not silent:
        print(f"üìã –û—á–µ—Ä–µ–¥—å –ø—Ä–æ–≤–µ—Ä–∫–∏ (API #{CURRENT_API_KEY_INDEX + 1}): {candidates}")
    
    for model_name in candidates:
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –º–æ–¥–µ–ª–∏ —Å –∏—Å—á–µ—Ä–ø–∞–Ω–Ω—ã–º–∏ –ª–∏–º–∏—Ç–∞–º–∏ –Ω–∞ —ç—Ç–æ–º –∫–ª—é—á–µ
        if MODEL_LIMITS.get(model_name, {}).get(CURRENT_API_KEY_INDEX, False):
            if not silent:
                print(f"‚è≠Ô∏è  {model_name} ‚Äî –ª–∏–º–∏—Ç –∏—Å—á–µ—Ä–ø–∞–Ω –Ω–∞ —ç—Ç–æ–º API –∫–ª—é—á–µ")
            continue
        
        if not silent:
            print(f"üëâ –¢–µ—Å—Ç: {model_name}...", end=" ")
        try:
            test_model = genai.GenerativeModel(
                model_name=model_name,
                generation_config=generation_config,
                system_instruction=SYSTEM_PROMPT_DEFAULT
            )
            # –ü–∏–Ω–≥
            response = await test_model.generate_content_async("ping")
            
            if response and response.text:
                if not silent:
                    print("‚úÖ –ñ–ò–í–ê–Ø! –ü–æ–¥–∫–ª—é—á–∞—é—Å—å.")
                ACTIVE_MODEL = test_model
                ACTIVE_MODEL_NAME = model_name
                return True
            
        except Exception as e:
            err = str(e)
            if "429" in err:
                if not silent:
                    print("‚ùå (429 –õ–∏–º–∏—Ç)")
                # –û—Ç–º–µ—á–∞–µ–º –ª–∏–º–∏—Ç –¥–ª—è —ç—Ç–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ —ç—Ç–æ–º –∫–ª—é—á–µ
                if model_name not in MODEL_LIMITS:
                    MODEL_LIMITS[model_name] = {}
                MODEL_LIMITS[model_name][CURRENT_API_KEY_INDEX] = True
                if not silent:
                    print(f"   üìù –ú–æ–¥–µ–ª—å {model_name} –∏—Å—á–µ—Ä–ø–∞–Ω–∞ –Ω–∞ API #{CURRENT_API_KEY_INDEX + 1}")
            elif "404" in err:
                if not silent:
                    print("‚ùå (404 –ù–µ—Ç –¥–æ—Å—Ç—É–ø–∞)")
            else:
                if not silent:
                    print(f"‚ùå ({err})")
    
    if not silent:
        print("üíÄ –í—Å–µ –º–æ–¥–µ–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã –Ω–∞ —ç—Ç–æ–º API –∫–ª—é—á–µ.")
    return False

async def is_addressed_to_bot(message: Message, bot_user: types.User):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –∞–¥—Ä–µ—Å–æ–≤–∞–Ω–æ –ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ –±–æ—Ç—É."""
    if message.chat.type == "private":
        return True
    if message.reply_to_message and message.reply_to_message.from_user.id == bot_user.id:
        return True
    if message.text and f"@{bot_user.username}" in message.text:
        return True
    if message.caption and f"@{bot_user.username}" in message.caption:
        return True
    return False

async def prepare_prompt_parts(message: Message, bot_user: types.User) -> Tuple[List, List]:
    """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç —á–∞—Å—Ç–∏ –ø—Ä–æ–º—Ç–∞ –∏ —Å–ø–∏—Å–æ–∫ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è."""
    prompt_parts = []
    temp_files_to_delete = []
    
    text_content = ""
    if message.text:
        text_content = message.text.replace(f"@{bot_user.username}", "").strip()
    elif message.caption:
        text_content = message.caption.replace(f"@{bot_user.username}", "").strip()
    
    if text_content:
        prompt_parts.append(text_content)
    
    if message.photo:
        photo_id = message.photo[-1].file_id
        file_info = await bot.get_file(photo_id)
        img_data = BytesIO()
        await bot.download_file(file_info.file_path, img_data)
        img_data.seek(0)
        image = Image.open(img_data)
        prompt_parts.append(image)
    
    if message.voice:
        file_id = message.voice.file_id
        file_info = await bot.get_file(file_id)
        with tempfile.NamedTemporaryFile(suffix=".ogg", delete=False) as temp_audio:
            await bot.download_file(file_info.file_path, destination=temp_audio.name)
            temp_path = temp_audio.name
        temp_files_to_delete.append(temp_path)
        
        uploaded_file = genai.upload_file(path=temp_path, mime_type="audio/ogg")
        while uploaded_file.state.name == "PROCESSING":
            await asyncio.sleep(1)
            uploaded_file = genai.get_file(uploaded_file.name)
        
        prompt_parts.append(uploaded_file)
        prompt_parts.append("–ü–æ—Å–ª—É—à–∞–π –∏ –æ—Ç–≤–µ—Ç—å.")
    
    return prompt_parts, temp_files_to_delete

async def process_with_retry(message: Message, bot_user: types.User, text_content: str, 
                             prompt_parts: List, status_message: Optional[Message] = None):
    """–ü—Ä–æ–±—É–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ —Å –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ–º –º–æ–¥–µ–ª–µ–π –∏ API –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏."""
    global ACTIVE_MODEL, ACTIVE_MODEL_NAME, CURRENT_API_KEY_INDEX
    
    temp_files_to_delete = []
    
    try:
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω—É–∂–Ω—ã–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º—Ç
        system_prompt = detect_system_prompt(text_content)
        
        if not prompt_parts:
            return
        
        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å —Å –Ω—É–∂–Ω—ã–º —Å–∏—Å—Ç–µ–º–Ω—ã–º –ø—Ä–æ–º—Ç–æ–º
        current_model = genai.GenerativeModel(
            model_name=ACTIVE_MODEL_NAME,
            generation_config=generation_config,
            system_instruction=system_prompt
        )
        
        response = await current_model.generate_content_async(prompt_parts)
        
        if response.text:
            await message.reply(response.text)
        else:
            await message.reply("...")
        
        return True
    
    except Exception as e:
        logging.error(f"Gen Error: {e}")
        error_str = str(e)
        
        if "429" in error_str or "quota" in error_str:
            # –õ–∏–º–∏—Ç –∏—Å—á–µ—Ä–ø–∞–Ω –Ω–∞ —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏ –∏ API
            if ACTIVE_MODEL_NAME not in MODEL_LIMITS:
                MODEL_LIMITS[ACTIVE_MODEL_NAME] = {}
            MODEL_LIMITS[ACTIVE_MODEL_NAME][CURRENT_API_KEY_INDEX] = True
            
            print(f"‚ö†Ô∏è –õ–∏–º–∏—Ç {ACTIVE_MODEL_NAME} –Ω–∞ API #{CURRENT_API_KEY_INDEX + 1}")
            
            # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –¥—Ä—É–≥—É—é –º–æ–¥–µ–ª—å –Ω–∞ —ç—Ç–æ–º –∂–µ –∫–ª—é—á–µ (—Ç–∏—Ö–æ)
            if await find_best_working_model(silent=True):
                print(f"‚úÖ –ù–∞—à–ª–∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—É—é –º–æ–¥–µ–ª—å: {ACTIVE_MODEL_NAME}")
                return await process_with_retry(message, bot_user, text_content, prompt_parts, status_message)
            
            # –ù–µ—Ç –¥—Ä—É–≥–∏—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ —ç—Ç–æ–º –∫–ª—é—á–µ, –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ –¥—Ä—É–≥–æ–π API
            print(f"üîÑ –ù–µ—Ç –º–æ–¥–µ–ª–µ–π –Ω–∞ API #{CURRENT_API_KEY_INDEX + 1}, –ø—Ä–æ–±—É–µ–º –¥—Ä—É–≥–æ–π...")
            if await switch_api_key(silent=True):
                print(f"‚úÖ –ü–µ—Ä–µ–∫–ª—é—á–∏–ª–∏—Å—å –Ω–∞ API #{CURRENT_API_KEY_INDEX + 1}, –º–æ–¥–µ–ª—å: {ACTIVE_MODEL_NAME}")
                return await process_with_retry(message, bot_user, text_content, prompt_parts, status_message)
            
            # –í—Å–µ –∏—Å—á–µ—Ä–ø–∞–Ω–æ
            await message.reply("‚ùå –ù–∞ —Å–µ–≥–æ–¥–Ω—è –ª–∏–º–∏—Ç—ã –∫–æ–Ω—á–∏–ª–∏—Å—å, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –∑–∞–≤—Ç—Ä–∞.")
            return False
        
        elif "404" in error_str:
            # –ú–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –∏—â–µ–º –¥—Ä—É–≥—É—é
            if await find_best_working_model(silent=True):
                print(f"‚úÖ –ü–µ—Ä–µ–∫–ª—é—á–∏–ª–∏—Å—å –Ω–∞ –º–æ–¥–µ–ª—å: {ACTIVE_MODEL_NAME}")
                return await process_with_retry(message, bot_user, text_content, prompt_parts, status_message)
            
            await message.reply("‚ùå –ù–∞ —Å–µ–≥–æ–¥–Ω—è –ª–∏–º–∏—Ç—ã –∫–æ–Ω—á–∏–ª–∏—Å—å, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –∑–∞–≤—Ç—Ä–∞.")
            return False
        
        else:
            await message.reply("‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏.")
            return False

# --- –•–ï–ù–î–õ–ï–†–´ ---
@dp.message(CommandStart())
async def command_start_handler(message: Message):
    api_info = f" (API #{CURRENT_API_KEY_INDEX + 1}/{len(GOOGLE_KEYS)})" if len(GOOGLE_KEYS) > 1 else ""
    status = f"‚úÖ –ú–æ–¥–µ–ª—å: `{ACTIVE_MODEL_NAME}`{api_info}" if ACTIVE_MODEL else "üíÄ –ù–µ—Ç —Å–≤—è–∑–∏ —Å AI"
    
    limits_info = ""
    if MODEL_LIMITS:
        limits_info = "\n\nüìä –ò—Å—á–µ—Ä–ø–∞–Ω–Ω—ã–µ –ª–∏–º–∏—Ç—ã:\n"
        for model, apis in MODEL_LIMITS.items():
            exhausted = [f"API #{k+1}" for k, v in apis.items() if v]
            if exhausted:
                limits_info += f"  ‚Ä¢ {model}: {', '.join(exhausted)}\n"
    
    await message.answer(f"ü§ñ **Bot Reloaded**\n{status}{limits_info}")

@dp.message()
async def main_handler(message: Message):
    global ACTIVE_MODEL, ACTIVE_MODEL_NAME
    
    # –ï—Å–ª–∏ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ –Ω–µ –Ω–∞—à–ª–∏ –º–æ–¥–µ–ª—å, –ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —Å–µ–π—á–∞—Å
    if not ACTIVE_MODEL:
        status_msg = await message.answer("‚è≥ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞...")
        if not await find_best_working_model(silent=True):
            # –ü—Ä–æ–±—É–µ–º –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å—Å—è –Ω–∞ –¥—Ä—É–≥–æ–π API –∫–ª—é—á
            if not await switch_api_key(silent=True):
                await status_msg.edit_text("‚ùå –ù–∞ —Å–µ–≥–æ–¥–Ω—è –ª–∏–º–∏—Ç—ã –∫–æ–Ω—á–∏–ª–∏—Å—å, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –∑–∞–≤—Ç—Ä–∞.")
                return
        
        await status_msg.delete()
    
    bot_user = await bot.get_me()
    
    if not await is_addressed_to_bot(message, bot_user):
        return
    
    await bot.send_chat_action(chat_id=message.chat.id, action="typing")
    
    try:
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–æ–º—Ç —á–∞—Å—Ç–∏
        text_content = ""
        if message.text:
            text_content = message.text.replace(f"@{bot_user.username}", "").strip()
        elif message.caption:
            text_content = message.caption.replace(f"@{bot_user.username}", "").strip()
        
        prompt_parts, temp_files_to_delete = await prepare_prompt_parts(message, bot_user)
        
        if not prompt_parts:
            return
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π/API
        await process_with_retry(message, bot_user, text_content, prompt_parts)
    
    except Exception as e:
        logging.error(f"Handler Error: {e}")
        await message.reply("‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏.")
    
    finally:
        # –û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        for f_path in temp_files_to_delete:
            try:
                os.remove(f_path)
            except:
                pass

# --- SERVER ---
@app.get("/")
async def root():
    api_info = f" (API #{CURRENT_API_KEY_INDEX + 1}/{len(GOOGLE_KEYS)})" if len(GOOGLE_KEYS) > 1 else ""
    return {
        "status": "Alive",
        "model": ACTIVE_MODEL_NAME,
        "api_key": CURRENT_API_KEY_INDEX + 1,
        "total_api_keys": len(GOOGLE_KEYS),
        "exhausted_limits": MODEL_LIMITS
    }

@app.get("/health")
async def health_check():
    return {"status": "ok"}

async def keep_alive_ping():
    if not RENDER_URL:
        return
    while True:
        await asyncio.sleep(300)
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(f"{RENDER_URL}/health") as resp:
                    logging.info(f"Ping: {resp.status}")
        except Exception:
            pass

async def start_bot():
    # –í—ã–±–∏—Ä–∞–µ–º –ø–µ—Ä–≤—ã–π –¥–æ—Å—Ç—É–ø–Ω—ã–π –∫–ª—é—á
    global CURRENT_API_KEY_INDEX
    for i, key in enumerate(GOOGLE_KEYS):
        try:
            genai.configure(api_key=key)
            CURRENT_API_KEY_INDEX = i
            print(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º API –∫–ª—é—á #{i + 1}")
            break
        except Exception as e:
            print(f"‚ö†Ô∏è API –∫–ª—é—á #{i + 1} –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
    
    await find_best_working_model()
    await bot.delete_webhook(drop_pending_updates=True)
    await dp.start_polling(bot)

async def start_server():
    config = uvicorn.Config(app, host="0.0.0.0", port=10000, log_level="error")
    server = uvicorn.Server(config)
    await server.serve()

async def main():
    await asyncio.gather(start_server(), start_bot(), keep_alive_ping())

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        pass
