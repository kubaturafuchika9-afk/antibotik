import os
import asyncio
import logging
import sys
import tempfile
from io import BytesIO

import uvicorn
from fastapi import FastAPI
import aiohttp
from PIL import Image

from aiogram import Bot, Dispatcher, types
from aiogram.enums import ParseMode
from aiogram.filters import CommandStart
from aiogram.types import Message
from aiogram.client.default import DefaultBotProperties

import google.generativeai as genai

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
TOKEN = os.getenv("TELEGRAM_TOKEN")
GOOGLE_KEY = os.getenv("GOOGLE_API_KEY")
RENDER_URL = os.getenv("RENDER_EXTERNAL_URL")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Google Gemini
genai.configure(api_key=GOOGLE_KEY)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ç–µ–∫—É—â–µ–π —Ä–∞–±–æ—á–µ–π –º–æ–¥–µ–ª–∏
ACTIVE_MODEL = None
ACTIVE_MODEL_NAME = "Searching..."

# --- –ù–ê–°–¢–†–û–ô–ö–ò –ì–ï–ù–ï–†–ê–¶–ò–ò ---
generation_config = {
  "temperature": 0.7,
  "top_p": 0.95,
  "top_k": 40,
  "max_output_tokens": 8192,
}

# --- –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –ë–û–¢–ê ---
bot = Bot(
    token=TOKEN, 
    default=DefaultBotProperties(parse_mode=ParseMode.MARKDOWN)
)
dp = Dispatcher()
app = FastAPI()

logging.basicConfig(level=logging.INFO, stream=sys.stdout)

# --- –£–ú–ù–ê–Ø –õ–û–ì–ò–ö–ê –ü–û–î–ë–û–†–ê –ú–û–î–ï–õ–ò ---

def get_dynamic_model_list():
    """–ó–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç —É Google —Å–ø–∏—Å–æ–∫ –í–°–ï–• –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —ç—Ç–æ–≥–æ –∫–ª—é—á–∞."""
    print("üì° –ó–∞–ø—Ä–∞—à–∏–≤–∞—é —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π —É Google API...")
    available_models = []
    try:
        for m in genai.list_models():
            # –ù–∞–º –Ω—É–∂–Ω—ã —Ç–æ–ª—å–∫–æ –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ —É–º–µ—é—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç (—á–∞—Ç)
            if 'generateContent' in m.supported_generation_methods:
                # –û—á–∏—â–∞–µ–º –∏–º—è –æ—Ç –ø—Ä–∏—Å—Ç–∞–≤–∫–∏ "models/"
                name = m.name.replace("models/", "")
                # –§–∏–ª—å—Ç—Ä—É–µ–º –º—É—Å–æ—Ä (—Ç–æ–ª—å–∫–æ gemini)
                if "gemini" in name:
                    available_models.append(name)
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞: {e}")
    
    # –•–ê–ö–ò: Google —á–∞—Å—Ç–æ —Å–∫—Ä—ã–≤–∞–µ—Ç —Å—Ç–∞—Ä—ã–µ —Ä–∞–±–æ—á–∏–µ –º–æ–¥–µ–ª–∏ –∏–∑ —Å–ø–∏—Å–∫–∞. 
    # –ú—ã –¥–æ–±–∞–≤–ª—è–µ–º –∏—Ö –≤—Ä—É—á–Ω—É—é, —á—Ç–æ–±—ã –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∏—Ö —Ç–æ–∂–µ.
    hardcoded_fallbacks = ["gemini-1.5-flash", "gemini-1.5-flash-latest", "gemini-1.5-pro"]
    for h in hardcoded_fallbacks:
        if h not in available_models:
            available_models.append(h)
            
    return list(set(available_models)) # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã

def sort_models_priority(models):
    """–°–æ—Ä—Ç–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª–∏: —Å–Ω–∞—á–∞–ª–∞ Lite/Flash (–±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ), –ø–æ—Ç–æ–º Pro, –ø–æ—Ç–æ–º Exp."""
    def score(name):
        s = 0
        if "lite" in name: s += 100      # Lite - —Å–∞–º—ã–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç (–æ–±—ã—á–Ω–æ –±–µ—Å–ø–ª–∞—Ç–Ω–æ)
        if "flash" in name: s += 50      # Flash - –±—ã—Å—Ç—Ä–æ –∏ –¥–µ—à–µ–≤–æ
        if "1.5" in name: s += 20        # 1.5 - —Å—Ç–∞–±–∏–ª—å–Ω–µ–µ —á–µ–º 2.0
        if "exp" in name: s += 10        # –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —á–∞—Å—Ç–æ —Ö–∞–ª—è–≤–Ω—ã–µ
        if "pro" in name: s -= 10        # Pro - —á–∞—Å—Ç–æ –ª–∏–º–∏—Ç–Ω—ã–µ
        if "latest" in name: s -= 5      # Latest - –Ω–µ–ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ
        return s

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é "–∫—Ä—É—Ç–æ—Å—Ç–∏" –¥–ª—è –Ω–∞—Å
    return sorted(models, key=score, reverse=True)

async def find_best_working_model():
    """–ü–µ—Ä–µ–±–∏—Ä–∞–µ—Ç –º–æ–¥–µ–ª–∏ –∏ –∏—â–µ—Ç –∂–∏–≤—É—é."""
    global ACTIVE_MODEL, ACTIVE_MODEL_NAME
    
    # 1. –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫
    candidates = get_dynamic_model_list()
    # 2. –°–æ—Ä—Ç–∏—Ä—É–µ–º
    candidates = sort_models_priority(candidates)
    
    print(f"üìã –ö–∞–Ω–¥–∏–¥–∞—Ç—ã (–≤ –ø–æ—Ä—è–¥–∫–µ –æ—á–µ—Ä–µ–¥–∏): {candidates}")
    
    for model_name in candidates:
        print(f"üëâ –¢–µ—Å—Ç–∏—Ä—É—é: {model_name}...", end=" ")
        try:
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
            test_model = genai.GenerativeModel(
                model_name=model_name,
                generation_config=generation_config,
                system_instruction="–¢—ã –ø–æ–ª–µ–∑–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫."
            )
            # –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å (–º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π)
            response = await test_model.generate_content_async("ping")
            
            if response and response.text:
                print("‚úÖ –ñ–ò–í–ê–Ø! –ë–µ—Ä–µ–º.")
                ACTIVE_MODEL = test_model
                ACTIVE_MODEL_NAME = model_name
                return True
                
        except Exception as e:
            err = str(e)
            if "429" in err:
                print("‚ùå (429 –õ–∏–º–∏—Ç)")
            elif "404" in err:
                print("‚ùå (404 –ù–µ –Ω–∞–π–¥–µ–Ω–∞)")
            elif "400" in err:
                print(f"‚ùå (400 –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞)")
            else:
                print(f"‚ùå (–û—à–∏–±–∫–∞: {err})")
    
    print("üíÄ –í–°–ï –ú–û–î–ï–õ–ò –ú–ï–†–¢–í–´. –ù—É–∂–µ–Ω –Ω–æ–≤—ã–π –∫–ª—é—á.")
    return False

# --- –õ–û–ì–ò–ö–ê –ë–û–¢–ê ---

async def is_addressed_to_bot(message: Message, bot_user: types.User):
    if message.chat.type == "private":
        return True
    if message.reply_to_message and message.reply_to_message.from_user.id == bot_user.id:
        return True
    if message.text and f"@{bot_user.username}" in message.text:
        return True
    if message.caption and f"@{bot_user.username}" in message.caption:
        return True
    return False

# --- –•–ï–ù–î–õ–ï–†–´ ---

@dp.message(CommandStart())
async def command_start_handler(message: Message):
    status = f"‚úÖ –†–∞–±–æ—Ç–∞—é –Ω–∞: `{ACTIVE_MODEL_NAME}`" if ACTIVE_MODEL else "üíÄ –ù–µ—Ç —Ä–∞–±–æ—á–∏—Ö –º–æ–¥–µ–ª–µ–π"
    await message.answer(f"ü§ñ **Auto-Discovery Bot**\n{status}")

@dp.message()
async def main_handler(message: Message):
    # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ, –ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —Å–µ–π—á–∞—Å (Lazy Loading)
    if not ACTIVE_MODEL:
        await message.answer("üîÑ –ò—â—É —Ä–∞–±–æ—á—É—é –º–æ–¥–µ–ª—å, –ø–æ–¥–æ–∂–¥–∏...")
        if not await find_best_working_model():
            await message.answer("‚ùå –ù–µ –Ω–∞—à–µ–ª —Ä–∞–±–æ—á–∏—Ö –º–æ–¥–µ–ª–µ–π. –ü—Ä–æ–≤–µ—Ä—å –∫–æ–Ω—Å–æ–ª—å.")
            return

    bot_user = await bot.get_me()
    
    if not await is_addressed_to_bot(message, bot_user):
        return

    await bot.send_chat_action(chat_id=message.chat.id, action="typing")

    prompt_parts = [] 
    temp_files_to_delete = []

    try:
        # –¢–ï–ö–°–¢
        text_content = ""
        if message.text:
            text_content = message.text.replace(f"@{bot_user.username}", "").strip()
        elif message.caption:
            text_content = message.caption.replace(f"@{bot_user.username}", "").strip()
        
        if text_content:
            prompt_parts.append(text_content)

        # –§–û–¢–û
        if message.photo:
            photo_id = message.photo[-1].file_id
            file_info = await bot.get_file(photo_id)
            img_data = BytesIO()
            await bot.download_file(file_info.file_path, img_data)
            img_data.seek(0)
            image = Image.open(img_data)
            prompt_parts.append(image)

        # –ì–û–õ–û–°–û–í–û–ï
        if message.voice:
            file_id = message.voice.file_id
            file_info = await bot.get_file(file_id)
            with tempfile.NamedTemporaryFile(suffix=".ogg", delete=False) as temp_audio:
                await bot.download_file(file_info.file_path, destination=temp_audio.name)
                temp_path = temp_audio.name
            temp_files_to_delete.append(temp_path)

            uploaded_file = genai.upload_file(path=temp_path, mime_type="audio/ogg")
            # –ñ–¥–µ–º –æ–±—Ä–∞–±–æ—Ç–∫–∏
            while uploaded_file.state.name == "PROCESSING":
                await asyncio.sleep(1)
                uploaded_file = genai.get_file(uploaded_file.name)

            prompt_parts.append(uploaded_file)
            prompt_parts.append("–ü–æ—Å–ª—É—à–∞–π –∞—É–¥–∏–æ –∏ –æ—Ç–≤–µ—Ç—å.")

        if not prompt_parts:
            await message.reply("–ü—É—Å—Ç–æ.")
            return

        # –ì–ï–ù–ï–†–ê–¶–ò–Ø
        response = await ACTIVE_MODEL.generate_content_async(prompt_parts)
        
        if response.text:
            await message.reply(response.text)
        else:
            await message.reply("...")

    except Exception as e:
        logging.error(f"Generation Error on {ACTIVE_MODEL_NAME}: {e}")
        # –ï—Å–ª–∏ —Ç–µ–∫—É—â–∞—è –º–æ–¥–µ–ª—å —É–º–µ—Ä–ª–∞ (429), –ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –Ω–æ–≤—É—é –Ω–∞ –ª–µ—Ç—É
        if "429" in str(e) or "404" in str(e):
             await message.reply(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å {ACTIVE_MODEL_NAME} –æ—Ç–≤–∞–ª–∏–ª–∞—Å—å. –ò—â—É –Ω–æ–≤—É—é...")
             if await find_best_working_model():
                 await message.reply(f"‚úÖ –ü–µ—Ä–µ–∫–ª—é—á–∏–ª—Å—è –Ω–∞ {ACTIVE_MODEL_NAME}. –ü–æ–≤—Ç–æ—Ä–∏ –∑–∞–ø—Ä–æ—Å.")
             else:
                 await message.reply("üíÄ –í—Å–µ –º–æ–¥–µ–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã.")
        else:
             await message.reply(f"–û—à–∏–±–∫–∞: {e}")
    
    finally:
        for f_path in temp_files_to_delete:
            try:
                os.remove(f_path)
            except:
                pass

# --- SERVER ---

@app.get("/")
async def root():
    return {"status": "Alive", "current_model": ACTIVE_MODEL_NAME}

@app.get("/health")
async def health_check():
    return {"status": "ok"}

async def keep_alive_ping():
    if not RENDER_URL:
        return
    while True:
        await asyncio.sleep(300)
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(f"{RENDER_URL}/health") as resp:
                    logging.info(f"Ping: {resp.status}")
        except Exception:
            pass

async def start_bot():
    # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–æ–∏—Å–∫ –º–æ–¥–µ–ª–∏ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
    await find_best_working_model()
    await bot.delete_webhook(drop_pending_updates=True)
    await dp.start_polling(bot)

async def start_server():
    config = uvicorn.Config(app, host="0.0.0.0", port=10000, log_level="error")
    server = uvicorn.Server(config)
    await server.serve()

async def main():
    await asyncio.gather(start_server(), start_bot(), keep_alive_ping())

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        pass
