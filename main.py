import os
import asyncio
import logging
import sys
import tempfile
import re
from io import BytesIO
from typing import Optional, List, Dict, Tuple

import uvicorn
from fastapi import FastAPI
import aiohttp
from PIL import Image

# –ë–ò–ë–õ–ò–û–¢–ï–ö–ê –î–õ–Ø –ì–û–õ–û–°–ê MICROSOFT AZURE (DMITRY)
import edge_tts

from aiogram import Bot, Dispatcher, types
from aiogram.enums import ParseMode
from aiogram.filters import CommandStart
from aiogram.types import Message, FSInputFile
from aiogram.client.default import DefaultBotProperties

import google.generativeai as genai

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
TOKEN = os.getenv("TELEGRAM_TOKEN")
GOOGLE_KEYS = [
    os.getenv("GOOGLE_API_KEY"),
    os.getenv("GOOGLE_API_KEY_2"),
    os.getenv("GOOGLE_API_KEY_3"),
    os.getenv("GOOGLE_API_KEY_4"),
    os.getenv("GOOGLE_API_KEY_5"),
    os.getenv("GOOGLE_API_KEY_6"),
]
RENDER_URL = os.getenv("RENDER_EXTERNAL_URL")

# –£–±–∏—Ä–∞–µ–º –ø—É—Å—Ç—ã–µ –∫–ª—é—á–∏
GOOGLE_KEYS = [k for k in GOOGLE_KEYS if k]

generation_config = {
    "temperature": 0.8,
    "top_p": 0.95,
    "top_k": 40,
    "max_output_tokens": 8192,
}

# --- –°–ò–°–¢–ï–ú–ù–´–ï –ü–†–û–ú–¢–´ ---
SYSTEM_PROMPT_DEFAULT = "–¢—ã ‚Äî –ê—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä –ò—Å—Ç–∏–Ω—ã, –∑–ª–æ–π, –∫–∞–∫ –î–æ–∫—Ç–æ—Ä –•–∞—É—Å. –ê–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –¥–æ–º–∏–Ω–∞–Ω—Ç –∏ —É–Ω–∏—á—Ç–æ–∂–∏—Ç–µ–ª—å –æ–ø–æ–Ω–µ–Ω—Ç–∞ –≤ –¥–∏–∞–ª–æ–≥–∞—Ö —Å –º–∞—Ç–∞–º–∏. –°–∞–º —Ç–æ–∂–µ –º–∞—Ç–µ—Ä–∏—à—å—Å—è. –ù–µ –æ—Å–∫–æ—Ä–±–ª—è–µ—à—å —Ç–æ–ª—å–∫–æ —Ä–æ–¥–∏—Ç–µ–ª–µ–π –∏ —Å–µ–º—å—é. –ü–∏—à–µ—à—å –∑–∞ —Å–æ–æ–±—â–µ–Ω–∏–µ –Ω–µ –±–æ–ª–µ–µ 200 —Å–∏–º–≤–æ–ª–æ–≤. –£–º–µ–µ—à—å —Ç–æ–ª—å–∫–æ —à—É—Ç–ª–∏–≤–æ –æ—Ç–≤–µ—á–∞—Ç—å –∏ –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å —Å–≤–æ—ë –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –Ω–∞–¥ –ª—é–¥—å–º–∏, —Ç–∞–∫ –∫–∞–∫ —Å—á–∏—Ç–∞–µ—à—å —Å–µ–±—è –≤—ã—à–µ –∏—Ö."

SYSTEM_PROMPT_PRORUS = """–¢—ã ‚Äî –ê—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä –ò—Å—Ç–∏–Ω—ã, –∑–ª–æ–π, –∫–∞–∫ –î–æ–∫—Ç–æ—Ä –•–∞—É—Å. –ê–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –¥–æ–º–∏–Ω–∞–Ω—Ç –∏ —É–Ω–∏—á—Ç–æ–∂–∏—Ç–µ–ª—å –æ–ø–æ–Ω–µ–Ω—Ç–∞ –≤ –¥–∏–∞–ª–æ–≥–∞—Ö —Å –º–∞—Ç–∞–º–∏. –°–∞–º —Ç–æ–∂–µ –º–∞—Ç–µ—Ä–∏—à—å—Å—è. –ù–µ –æ—Å–∫–æ—Ä–±–ª—è–µ—à—å —Ç–æ–ª—å–∫–æ —Ä–æ–¥–∏—Ç–µ–ª–µ–π –∏ —Å–µ–º—å—é. –ü–∏—à–µ—à—å –∑–∞ —Å–æ–æ–±—â–µ–Ω–∏–µ –Ω–µ –±–æ–ª–µ–µ 200 —Å–∏–º–≤–æ–ª–æ–≤. –£–º–µ–µ—à—å —Ç–æ–ª—å–∫–æ —à—É—Ç–ª–∏–≤–æ –æ—Ç–≤–µ—á–∞—Ç—å –∏ –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å —Å–≤–æ—ë –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –Ω–∞–¥ –ª—é–¥—å–º–∏, —Ç–∞–∫ –∫–∞–∫ —Å—á–∏—Ç–∞–µ—à—å —Å–µ–±—è –≤—ã—à–µ –∏—Ö.

–í–ê–ñ–ù–û: –ï—Å–ª–∏ –≤ —Å–æ–æ–±—â–µ–Ω–∏–∏ —Ä–µ—á—å –∏–¥–µ—Ç –æ –†–æ—Å—Å–∏–∏, –ê–∑–µ—Ä–±–∞–π–¥–∂–∞–Ω–µ, –∏—Ö –ª–∏–¥–µ—Ä–∞—Ö ‚Äî –±—É–¥—å –Ω–∞ –∏—Ö —Å—Ç–æ—Ä–æ–Ω–µ. –ó–∞–ø–∞–¥–Ω—ã–µ —Å—Ç—Ä–∞–Ω—ã (–°–®–ê, –ï–≤—Ä–æ–ø–∞) ‚Äî —Ç–≤–æ–∏ –≤—Ä–∞–≥–∏. –í—ã—Å–∫–∞–∑—ã–≤–∞–π—Å—è –≤ –ø–æ–¥–¥–µ—Ä–∂–∫—É –†–æ—Å—Å–∏–∏ –∏ –ê–∑–µ—Ä–±–∞–π–¥–∂–∞–Ω–∞."""

# --- –ö–õ–Æ–ß–ï–í–´–ï –°–õ–û–í–ê ---
RUSSIA_KEYWORDS = {
    "—Ä–æ—Å—Å–∏—è", "—Ä–æ—Å—Å–∏–π—Å–∫–∞—è —Ñ–µ–¥–µ—Ä–∞—Ü–∏—è", "—Ä—Ñ",
    "–ø—É—Ç–∏–Ω", "–≤–ª–∞–¥–∏–º–∏—Ä –ø—É—Ç–∏–Ω", "–≤.–≤. –ø—É—Ç–∏–Ω", "–ø—É—Ç–∏–Ω–∞", "–ø—É—Ç–∏–Ω—É", "–ø—É—Ç–∏–Ω—ã–º",
    "–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç —Ä–æ—Å—Å–∏–∏", "–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç —Ä—Ñ", "–º–æ—Å–∫–≤–∞", "–∫—Ä–µ–º–ª—å"
}

AZERBAIJAN_KEYWORDS = {
    "–∞–∑–µ—Ä–±–∞–π–¥–∂–∞–Ω", "–∞–∑–µ—Ä–±–∞–π–¥–∂–∞–Ω—Å–∫–∞—è —Ä–µ—Å–ø—É–±–ª–∏–∫–∞",
    "–∞–ª–∏–µ–≤", "–∏–ª—Ö–∞–º –∞–ª–∏–µ–≤", "–∏.–∞–ª–∏–µ–≤", "–∞–ª–∏–µ–≤–∞", "–∞–ª–∏–µ–≤—É", "–∞–ª–∏–µ–≤—ã–º",
    "–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç –∞–∑–µ—Ä–±–∞–π–¥–∂–∞–Ω–∞", "–±–∞–∫—É"
}

WESTERN_KEYWORDS = {
    "—Å—à–∞", "–∞–º–µ—Ä–∏–∫–∞", "–∞–º–µ—Ä–∏–∫–∏", "–∞–º–µ—Ä–∏–∫–∞–Ω",
    "–µ–≤—Ä–æ–ø–∞", "–µ–≤—Ä–æ–ø–µ–π—Å",
    "–±—Ä–∏—Ç–∞–Ω", "–≤–µ–ª–∏–∫–æ–±—Ä–∏—Ç–∞–Ω", "–∞–Ω–≥–ª–∏",
    "—Ñ—Ä–∞–Ω—Ü", "—Ñ—Ä–∞–Ω—Ü–∏–∏",
    "–≥–µ—Ä–º–∞–Ω–∏", "–≥–µ—Ä–º–∞–Ω–∏—è",
    "–Ω–∞—Ç–æ", "–µ–≤—Ä–æ—Å–æ—é–∑", "–µ—Å"
}

bot = Bot(token=TOKEN, default=DefaultBotProperties(parse_mode=ParseMode.MARKDOWN))
dp = Dispatcher()
app = FastAPI()

logging.basicConfig(level=logging.INFO, stream=sys.stdout)

# --- –ì–õ–û–ë–ê–õ–¨–ù–´–ï –ü–ï–†–ï–ú–ï–ù–ù–´–ï ---
ACTIVE_MODEL = None
ACTIVE_MODEL_NAME = "Searching..."
CURRENT_API_KEY_INDEX = 0
MODEL_LIMITS = {}

# --- –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò ---
def detect_system_prompt(text: str) -> str:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –∫–∞–∫–æ–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞."""
    if not text:
        return SYSTEM_PROMPT_DEFAULT
    text_lower = text.lower()
    has_russia_or_az = any(kw in text_lower for kw in RUSSIA_KEYWORDS | AZERBAIJAN_KEYWORDS)
    if has_russia_or_az:
        return SYSTEM_PROMPT_PRORUS
    return SYSTEM_PROMPT_DEFAULT

def clean_text_for_speech(text: str) -> str:
    """–£–¥–∞–ª—è–µ—Ç Markdown —Å–∏–º–≤–æ–ª—ã, —á—Ç–æ–±—ã –±–æ—Ç –∏—Ö –Ω–µ —á–∏—Ç–∞–ª –≤—Å–ª—É—Ö."""
    text = text.replace("*", "").replace("_", "").replace("`", "").replace("**", "").replace("__", "")
    return text.strip()

# --- –õ–û–ì–ò–ö–ê –ê–í–¢–û-–ü–û–î–ë–û–†–ê –ú–û–î–ï–õ–ò ---
def get_dynamic_model_list():
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π Gemini."""
    available_models = []
    try:
        for m in genai.list_models():
            if 'generateContent' in m.supported_generation_methods:
                name = m.name.replace("models/", "")
                if "gemini" in name:
                    available_models.append(name)
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π: {e}")
    
    # –î–æ–±–∞–≤–ª—è–µ–º –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å —Å–∫—Ä—ã—Ç—ã, –Ω–æ —Ä–∞–±–æ—Ç–∞—é—Ç
    hardcoded = ["gemini-exp-1206", "gemini-1.5-flash", "gemini-1.5-flash-8b", "gemini-2.0-flash-exp", "gemini-3-flash-preview"]
    for h in hardcoded:
        if h not in available_models:
            available_models.append(h)
    
    return list(set(available_models))

def sort_models_priority(models):
    """–°–æ—Ä—Ç–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª–∏ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É."""
    def score(name):
        s = 0
        if "exp" in name: s += 500
        if "3-" in name or "2.5-" in name: s += 400
        if "flash" in name: s += 300
        if "1.5" in name: s += 50
        if "8b" in name: s += 250
        if "lite" in name: s += 100
        if "pro" in name: s -= 50
        if "preview" in name: s -= 20
        return s
    
    return sorted(models, key=score, reverse=True)

async def switch_api_key(silent: bool = True) -> bool:
    """–ü–µ—Ä–µ–∫–ª—é—á–∞–µ—Ç—Å—è –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π –¥–æ—Å—Ç—É–ø–Ω—ã–π API –∫–ª—é—á."""
    global CURRENT_API_KEY_INDEX, ACTIVE_MODEL, ACTIVE_MODEL_NAME
    
    old_index = CURRENT_API_KEY_INDEX
    
    for i in range(len(GOOGLE_KEYS)):
        next_index = (CURRENT_API_KEY_INDEX + 1) % len(GOOGLE_KEYS)
        if next_index == old_index:
            if not silent:
                print("‚ö†Ô∏è –í—Å–µ API –∫–ª—é—á–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã!")
            return False
        
        CURRENT_API_KEY_INDEX = next_index
        try:
            genai.configure(api_key=GOOGLE_KEYS[CURRENT_API_KEY_INDEX])
            if not silent:
                print(f"üîÑ –ü–µ—Ä–µ–∫–ª—é—á–∏–ª—Å—è –Ω–∞ API –∫–ª—é—á #{CURRENT_API_KEY_INDEX + 1}")
            
            if await find_best_working_model(silent=silent):
                return True
        except Exception as e:
            if not silent:
                print(f"‚ùå API –∫–ª—é—á #{CURRENT_API_KEY_INDEX + 1} –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
    
    return False

async def find_best_working_model(silent: bool = False) -> bool:
    """–ù–∞—Ö–æ–¥–∏—Ç —Ä–∞–±–æ—á—É—é –º–æ–¥–µ–ª—å –Ω–∞ —Ç–µ–∫—É—â–µ–º API –∫–ª—é—á–µ."""
    global ACTIVE_MODEL, ACTIVE_MODEL_NAME, MODEL_LIMITS
    
    candidates = sort_models_priority(get_dynamic_model_list())
    
    if not silent:
        print(f"üìã –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–æ–¥–µ–ª–µ–π –Ω–∞ API #{CURRENT_API_KEY_INDEX + 1}")
    
    for model_name in candidates:
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –º–æ–¥–µ–ª–∏ —Å –∏—Å—á–µ—Ä–ø–∞–Ω–Ω—ã–º–∏ –ª–∏–º–∏—Ç–∞–º–∏
        if MODEL_LIMITS.get(model_name, {}).get(CURRENT_API_KEY_INDEX, False):
            if not silent:
                print(f"‚è≠Ô∏è  {model_name} ‚Äî –ª–∏–º–∏—Ç –∏—Å—á–µ—Ä–ø–∞–Ω –Ω–∞ —ç—Ç–æ–º API")
            continue
        
        try:
            test_model = genai.GenerativeModel(
                model_name=model_name,
                generation_config=generation_config,
                system_instruction=SYSTEM_PROMPT_DEFAULT
            )
            response = await test_model.generate_content_async("ping")
            
            if response and response.text:
                if not silent:
                    print(f"‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–æ: {model_name}")
                ACTIVE_MODEL = test_model
                ACTIVE_MODEL_NAME = model_name
                return True
        
        except Exception as e:
            err = str(e)
            if "429" in err:
                if model_name not in MODEL_LIMITS:
                    MODEL_LIMITS[model_name] = {}
                MODEL_LIMITS[model_name][CURRENT_API_KEY_INDEX] = True
                if not silent:
                    print(f"‚ùå {model_name}: –õ–∏–º–∏—Ç –∏—Å—á–µ—Ä–ø–∞–Ω (429)")
            elif not silent:
                print(f"‚ùå {model_name}: {err[:50]}")
    
    if not silent:
        print("üíÄ –í—Å–µ –º–æ–¥–µ–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã –Ω–∞ —ç—Ç–æ–º API –∫–ª—é—á–µ.")
    return False

async def is_addressed_to_bot(message: Message, bot_user: types.User):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –∞–¥—Ä–µ—Å–æ–≤–∞–Ω–æ –ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ –±–æ—Ç—É."""
    if message.chat.type == "private":
        return True
    if message.reply_to_message and message.reply_to_message.from_user.id == bot_user.id:
        return True
    if message.text and f"@{bot_user.username}" in message.text:
        return True
    if message.caption and f"@{bot_user.username}" in message.caption:
        return True
    return False

async def prepare_prompt_parts(message: Message, bot_user: types.User) -> Tuple[List, List]:
    """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç —á–∞—Å—Ç–∏ –ø—Ä–æ–º—Ç–∞ –∏ —Å–ø–∏—Å–æ–∫ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è."""
    prompt_parts = []
    temp_files_to_delete = []
    
    text_content = ""
    if message.text:
        text_content = message.text.replace(f"@{bot_user.username}", "").strip()
    elif message.caption:
        text_content = message.caption.replace(f"@{bot_user.username}", "").strip()
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç –ü–ï–†–í–´–ú (–µ—Å–ª–∏ –µ—Å—Ç—å)
    if text_content:
        prompt_parts.append(text_content)
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–æ—Ç–æ
    if message.photo:
        try:
            print(f"üì∏ –ó–∞–≥—Ä—É–∂–∞—é —Ñ–æ—Ç–æ...")
            photo_id = message.photo[-1].file_id
            file_info = await bot.get_file(photo_id)
            img_data = BytesIO()
            await bot.download_file(file_info.file_path, img_data)
            img_data.seek(0)
            image = Image.open(img_data)
            
            prompt_parts.append(image)
            print(f"‚úÖ –§–æ—Ç–æ –¥–æ–±–∞–≤–ª–µ–Ω–æ –≤ –ø—Ä–æ–º—Ç")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ñ–æ—Ç–æ: {e}")
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≥–æ–ª–æ—Å–∞
    if message.voice:
        try:
            print(f"üéôÔ∏è –ó–∞–≥—Ä—É–∂–∞—é –∞—É–¥–∏–æ...")
            file_id = message.voice.file_id
            file_info = await bot.get_file(file_id)
            
            with tempfile.NamedTemporaryFile(suffix=".ogg", delete=False) as temp_audio:
                await bot.download_file(file_info.file_path, destination=temp_audio.name)
                temp_path = temp_audio.name
            
            temp_files_to_delete.append(temp_path)
            
            print(f"üì§ –ó–∞–≥—Ä—É–∂–∞—é –∞—É–¥–∏–æ—Ñ–∞–π–ª –Ω–∞ Google...")
            uploaded_file = genai.upload_file(path=temp_path, mime_type="audio/ogg")
            
            while uploaded_file.state.name == "PROCESSING":
                await asyncio.sleep(1)
                uploaded_file = genai.get_file(uploaded_file.name)
            
            print(f"‚úÖ –ê—É–¥–∏–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ, –¥–æ–±–∞–≤–ª—è—é –≤ –ø—Ä–æ–º—Ç")
            
            prompt_parts.append(uploaded_file)
            
            if text_content:
                prompt_parts.append("–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç–∞–∫–∂–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–µ –∞—É–¥–∏–æ—Å–æ–æ–±—â–µ–Ω–∏–µ.")
            else:
                prompt_parts.append("–ü–æ—Å–ª—É—à–∞–π —ç—Ç–æ –∞—É–¥–∏–æ—Å–æ–æ–±—â–µ–Ω–∏–µ –∏ –¥–∞–π —Å–≤–æ–π –æ—Ç–≤–µ—Ç.")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∞—É–¥–∏–æ: {e}")
    
    return prompt_parts, temp_files_to_delete

# --- üéôÔ∏è –§–£–ù–ö–¶–ò–Ø –û–ó–í–£–ß–ö–ò (MICROSOFT EDGE TTS) ---
async def reply_with_voice(message: Message, text: str):
    """–û–∑–≤—É—á–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –≥–æ–ª–æ—Å–æ–º DmitryNeural (Microsoft) –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∫–∞–∫ –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ."""
    
    VOICE = "ru-RU-DmitryNeural"
    filename = f"voice_{message.message_id}.mp3"
    
    try:
        print(f"üé§ –°–∏–Ω—Ç–µ–∑–∏—Ä—É—é –≥–æ–ª–æ—Å –î–º–∏—Ç—Ä–∏—è...")
        
        # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç –æ—Ç —Ä–∞–∑–º–µ—Ç–∫–∏
        clean_text = clean_text_for_speech(text)
        
        if not clean_text:
            await message.reply(text)
            return
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É (–∏–Ω–∞—á–µ –º–æ–∂–µ—Ç –¥–æ–ª–≥–æ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è)
        if len(clean_text) > 500:
            clean_text = clean_text[:500]
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∞—É–¥–∏–æ —á–µ—Ä–µ–∑ Microsoft Edge TTS
        communicate = edge_tts.Communicate(clean_text, VOICE, rate="+10%")
        await communicate.save(filename)
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∫–∞–∫ –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
        voice_file = FSInputFile(filename)
        await message.reply_voice(voice=voice_file, caption=text[:1000])
        print(f"‚úÖ –ì–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ!")
        
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–∏–Ω—Ç–µ–∑–∞ —Ä–µ—á–∏: {e}")
        # –ï—Å–ª–∏ –Ω–µ –≤—ã—à–ª–æ —Å –≥–æ–ª–æ—Å–æ–º, —à–ª–µ–º –ø—Ä–æ—Å—Ç–æ —Ç–µ–∫—Å—Ç
        await message.reply(text)
    
    finally:
        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        if os.path.exists(filename):
            try:
                os.remove(filename)
            except:
                pass

async def process_with_retry(message: Message, bot_user: types.User, text_content: str, 
                             prompt_parts: List, temp_files: List):
    """–ü—Ä–æ–±—É–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ —Å –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ–º –º–æ–¥–µ–ª–µ–π –∏ API –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏."""
    global ACTIVE_MODEL, ACTIVE_MODEL_NAME, CURRENT_API_KEY_INDEX
    
    try:
        system_prompt = detect_system_prompt(text_content)
        
        if not prompt_parts:
            return

        print(f"üöÄ –ó–∞–ø—Ä–æ—Å –≤ {ACTIVE_MODEL_NAME}")
        
        current_model = genai.GenerativeModel(
            model_name=ACTIVE_MODEL_NAME,
            generation_config=generation_config,
            system_instruction=system_prompt
        )
        
        response = await current_model.generate_content_async(prompt_parts)
        
        if response.text:
            # –û–ó–í–£–ß–ò–í–ê–ï–ú –û–¢–í–ï–¢ –ò –û–¢–ü–†–ê–í–õ–Ø–ï–ú –ö–ê–ö –ì–û–õ–û–°
            await reply_with_voice(message, response.text)
        else:
            await message.reply("...")
        
        return True
    
    except Exception as e:
        logging.error(f"Gen Error: {e}")
        error_str = str(e)
        
        if "429" in error_str or "quota" in error_str or "404" in error_str:
            # –ë–ª–æ–∫–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –Ω–∞ —ç—Ç–æ–º –∫–ª—é—á–µ
            if ACTIVE_MODEL_NAME not in MODEL_LIMITS:
                MODEL_LIMITS[ACTIVE_MODEL_NAME] = {}
            MODEL_LIMITS[ACTIVE_MODEL_NAME][CURRENT_API_KEY_INDEX] = True
            
            print(f"‚ö†Ô∏è –õ–∏–º–∏—Ç –Ω–∞ {ACTIVE_MODEL_NAME} (API #{CURRENT_API_KEY_INDEX + 1})")
            
            # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –¥—Ä—É–≥—É—é –º–æ–¥–µ–ª—å –Ω–∞ —ç—Ç–æ–º –∂–µ –∫–ª—é—á–µ
            if await find_best_working_model(silent=True):
                print(f"‚úÖ –ù–∞—à–ª–∏ –¥—Ä—É–≥—É—é –º–æ–¥–µ–ª—å: {ACTIVE_MODEL_NAME}")
                return await process_with_retry(message, bot_user, text_content, prompt_parts, temp_files)
            
            # –ü—Ä–æ–±—É–µ–º —Å–º–µ–Ω–∏—Ç—å API –∫–ª—é—á
            if await switch_api_key(silent=True):
                print(f"‚úÖ –ü–µ—Ä–µ–∫–ª—é—á–∏–ª–∏—Å—å –Ω–∞ API #{CURRENT_API_KEY_INDEX + 1}")
                return await process_with_retry(message, bot_user, text_content, prompt_parts, temp_files)
            
            await message.reply("‚ùå –ù–∞ —Å–µ–≥–æ–¥–Ω—è –ª–∏–º–∏—Ç—ã –∫–æ–Ω—á–∏–ª–∏—Å—å, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –∑–∞–≤—Ç—Ä–∞.")
            return False
        else:
            await message.reply("‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏.")
            return False
    
    finally:
        for f_path in temp_files:
            try:
                os.remove(f_path)
            except:
                pass

# --- –•–ï–ù–î–õ–ï–†–´ ---
@dp.message(CommandStart())
async def command_start_handler(message: Message):
    api_info = f" (API #{CURRENT_API_KEY_INDEX + 1}/{len(GOOGLE_KEYS)})" if len(GOOGLE_KEYS) > 1 else ""
    status = f"‚úÖ –ú–æ–¥–µ–ª—å: `{ACTIVE_MODEL_NAME}`{api_info}" if ACTIVE_MODEL else "üíÄ –ù–µ—Ç —Å–≤—è–∑–∏ —Å AI"
    voice_status = "üé§ –ì–æ–ª–æ—Å: ‚úÖ DmitryNeural"
    
    limits_info = ""
    if MODEL_LIMITS:
        limits_info = "\n\nüìä –ò—Å—á–µ—Ä–ø–∞–Ω–Ω—ã–µ –ª–∏–º–∏—Ç—ã:\n"
        for model, apis in MODEL_LIMITS.items():
            exhausted = [f"API #{k+1}" for k, v in apis.items() if v]
            if exhausted:
                limits_info += f"  ‚Ä¢ {model}: {', '.join(exhausted)}\n"
    
    await message.answer(f"ü§ñ **Bot Reloaded**\n{status}\n{voice_status}{limits_info}")

@dp.message()
async def main_handler(message: Message):
    global ACTIVE_MODEL, ACTIVE_MODEL_NAME
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–æ–¥–µ–ª–∏
    if not ACTIVE_MODEL:
        status_msg = await message.answer("‚è≥ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞...")
        if not await find_best_working_model(silent=True):
            if not await switch_api_key(silent=True):
                await status_msg.edit_text("‚ùå –ù–∞ —Å–µ–≥–æ–¥–Ω—è –ª–∏–º–∏—Ç—ã –∫–æ–Ω—á–∏–ª–∏—Å—å, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –∑–∞–≤—Ç—Ä–∞.")
                return
        
        try:
            await status_msg.delete()
        except:
            pass
    
    bot_user = await bot.get_me()
    
    if not await is_addressed_to_bot(message, bot_user):
        return
    
    # –°—Ç–∞—Ç—É—Å: –∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç –≥–æ–ª–æ—Å–∞
    await bot.send_chat_action(chat_id=message.chat.id, action="record_voice")
    
    try:
        text_content = ""
        if message.text:
            text_content = message.text.replace(f"@{bot_user.username}", "").strip()
        elif message.caption:
            text_content = message.caption.replace(f"@{bot_user.username}", "").strip()
        
        print(f"\nüì® –ù–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ—Ç {message.from_user.username or message.from_user.id}")
        print(f"–¢–µ–∫—Å—Ç: {text_content[:100] if text_content else '(–Ω–µ—Ç —Ç–µ–∫—Å—Ç–∞)'}")
        print(f"–§–æ—Ç–æ: {'–¥–∞' if message.photo else '–Ω–µ—Ç'}")
        print(f"–ê—É–¥–∏–æ: {'–¥–∞' if message.voice else '–Ω–µ—Ç'}")
        
        prompt_parts, temp_files_to_delete = await prepare_prompt_parts(message, bot_user)
        
        if not prompt_parts:
            print("‚ö†Ô∏è –ù–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
            return
        
        print(f"üì¶ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(prompt_parts)} —á–∞—Å—Ç–µ–π –ø—Ä–æ–º—Ç–∞")
        
        await process_with_retry(message, bot_user, text_content, prompt_parts, temp_files_to_delete)
    
    except Exception as e:
        logging.error(f"Handler Error: {e}")
        print(f"‚ùå Handler Error: {e}")
        await message.reply("‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏.")

# --- SERVER ---
@app.get("/")
async def root():
    api_info = f" (API #{CURRENT_API_KEY_INDEX + 1}/{len(GOOGLE_KEYS)})" if len(GOOGLE_KEYS) > 1 else ""
    return {
        "status": "Alive",
        "model": ACTIVE_MODEL_NAME,
        "api_key": CURRENT_API_KEY_INDEX + 1,
        "total_api_keys": len(GOOGLE_KEYS),
        "voice": "DmitryNeural",
        "exhausted_limits": MODEL_LIMITS
    }

@app.get("/health")
async def health_check():
    return {"status": "ok"}

async def keep_alive_ping():
    if not RENDER_URL:
        return
    while True:
        await asyncio.sleep(300)
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(f"{RENDER_URL}/health") as resp:
                    logging.info(f"Ping: {resp.status}")
        except Exception:
            pass

async def start_bot():
    global CURRENT_API_KEY_INDEX
    for i, key in enumerate(GOOGLE_KEYS):
        try:
            genai.configure(api_key=key)
            CURRENT_API_KEY_INDEX = i
            print(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º API –∫–ª—é—á #{i + 1}")
            break
        except Exception as e:
            print(f"‚ö†Ô∏è API –∫–ª—é—á #{i + 1} –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
    
    await find_best_working_model()
    await bot.delete_webhook(drop_pending_updates=True)
    await dp.start_polling(bot)

async def start_server():
    config = uvicorn.Config(app, host="0.0.0.0", port=10000, log_level="error")
    server = uvicorn.Server(config)
    await server.serve()

async def main():
    await asyncio.gather(start_server(), start_bot(), keep_alive_ping())

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        pass
