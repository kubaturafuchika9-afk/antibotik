import os
import asyncio
import logging
import sys
import tempfile
from io import BytesIO

import uvicorn
from fastapi import FastAPI
import aiohttp
from PIL import Image

from aiogram import Bot, Dispatcher, types
from aiogram.enums import ParseMode
from aiogram.filters import CommandStart
from aiogram.types import Message
from aiogram.client.default import DefaultBotProperties

import google.generativeai as genai

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
TOKEN = os.getenv("TELEGRAM_TOKEN")
GOOGLE_KEY = os.getenv("GOOGLE_API_KEY")
RENDER_URL = os.getenv("RENDER_EXTERNAL_URL")

genai.configure(api_key=GOOGLE_KEY)

ACTIVE_MODEL = None
ACTIVE_MODEL_NAME = "Searching..."

generation_config = {
  "temperature": 0.7,
  "top_p": 0.95,
  "top_k": 40,
  "max_output_tokens": 8192,
}

bot = Bot(token=TOKEN, default=DefaultBotProperties(parse_mode=ParseMode.MARKDOWN))
dp = Dispatcher()
app = FastAPI()

logging.basicConfig(level=logging.INFO, stream=sys.stdout)

# --- –õ–û–ì–ò–ö–ê –ü–†–ò–û–†–ò–¢–ï–¢–û–í (–ò–°–ü–†–ê–í–õ–ï–ù–ê) ---

def get_dynamic_model_list():
    print("üì° –ó–∞–ø—Ä–∞—à–∏–≤–∞—é —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π...")
    available_models = []
    try:
        for m in genai.list_models():
            if 'generateContent' in m.supported_generation_methods:
                name = m.name.replace("models/", "")
                if "gemini" in name:
                    available_models.append(name)
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–ø–∏—Å–∫–∞: {e}")
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Å–∫—Ä—ã—Ç—ã–µ, –∫–æ—Ç–æ—Ä—ã–µ —Ö–æ—Ç–∏–º –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ
    # gemini-1.5-flash-8b - —ç—Ç–æ –Ω–æ–≤–∞—è –ª–µ–≥–∫–∞—è –º–æ–¥–µ–ª—å, —á–∞—Å—Ç–æ —Å —Ö–æ—Ä–æ—à–∏–º –ª–∏–º–∏—Ç–æ–º
    hardcoded = ["gemini-exp-1206", "gemini-1.5-flash", "gemini-1.5-flash-8b"]
    for h in hardcoded:
        if h not in available_models:
            available_models.append(h)
            
    return list(set(available_models))

def sort_models_priority(models):
    """
    –ó–¥–µ—Å—å –º—ã –∑–∞–¥–∞–µ–º '–≤–∫—É—Å–Ω–æ—Å—Ç—å' –º–æ–¥–µ–ª–∏.
    –ß–µ–º –±–æ–ª—å—à–µ –±–∞–ª–ª–æ–≤, —Ç–µ–º —Ä–∞–Ω—å—à–µ –±–æ—Ç –µ—ë –ø–æ–ø—Ä–æ–±—É–µ—Ç.
    """
    def score(name):
        s = 0
        # 1. –°–ê–ú–´–ô –¢–û–ü: –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ (–æ–±—ã—á–Ω–æ –±–µ–∑–ª–∏–º–∏—Ç –¥–ª—è —Ç–µ—Å—Ç–æ–≤)
        if "exp" in name: s += 500
        
        # 2. –ù–∞–¥–µ–∂–Ω—ã–µ —Ä–∞–±–æ—á–∏–µ –ª–æ—à–∞–¥–∫–∏
        if "1.5-flash" in name: s += 300
        
        # 3. –ù–æ–≤–∞—è —Å—É–ø–µ—Ä-–ª–µ–≥–∫–∞—è (–¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –¥–µ—à–µ–≤–æ–π)
        if "8b" in name: s += 250
        
        # 4. Lite –≤–µ—Ä—Å–∏–∏ (–∫–∞–∫ –º—ã –≤—ã—è—Å–Ω–∏–ª–∏, 2.0-lite –º–æ–∂–µ—Ç –±—ã—Ç—å —Å –ø–æ–¥–≤–æ—Ö–æ–º, –ø–æ—ç—Ç–æ–º—É –Ω–∏–∂–µ)
        if "lite" in name: s += 100
        
        # –®–¢–†–ê–§–´
        if "pro" in name: s -= 50        # Pro –±—ã—Å—Ç—Ä–æ –∫–æ–Ω—á–∞–µ—Ç—Å—è
        if "preview" in name: s -= 20    # Preview —á–∞—Å—Ç–æ –∏–º–µ—é—Ç –ª–∏–º–∏—Ç 20/–¥–µ–Ω—å (–∫–∞–∫ —Ç—ã –∑–∞–º–µ—Ç–∏–ª)
        
        return s

    return sorted(models, key=score, reverse=True)

async def find_best_working_model():
    global ACTIVE_MODEL, ACTIVE_MODEL_NAME
    
    candidates = get_dynamic_model_list()
    candidates = sort_models_priority(candidates)
    
    print(f"üìã –û—á–µ—Ä–µ–¥—å –ø—Ä–æ–≤–µ—Ä–∫–∏: {candidates}")
    
    for model_name in candidates:
        print(f"üëâ –¢–µ—Å—Ç: {model_name}...", end=" ")
        try:
            test_model = genai.GenerativeModel(
                model_name=model_name,
                generation_config=generation_config
            )
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º "–ø–∏–Ω–≥"
            response = await test_model.generate_content_async("ping")
            
            if response and response.text:
                print("‚úÖ –ñ–ò–í–ê–Ø! –ü–æ–¥–∫–ª—é—á–∞—é—Å—å.")
                ACTIVE_MODEL = test_model
                ACTIVE_MODEL_NAME = model_name
                
                # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–∏—Å—Ç–µ–º–Ω—É—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é —É–∂–µ –¥–ª—è —Ä–∞–±–æ—á–µ–π –º–æ–¥–µ–ª–∏
                ACTIVE_MODEL = genai.GenerativeModel(
                    model_name=model_name,
                    generation_config=generation_config,
                    system_instruction="–¢—ã –ø–æ–ª–µ–∑–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫ –≤ Telegram. –¢—ã —É–º–µ–µ—à—å —Å–ª—É—à–∞—Ç—å –≥–æ–ª–æ—Å–æ–≤—ã–µ –∏ —Å–º–æ—Ç—Ä–µ—Ç—å —Ñ–æ—Ç–æ. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ, –µ–º–∫–æ –∏ —Å —é–º–æ—Ä–æ–º."
                )
                return True
                
        except Exception as e:
            err = str(e)
            if "429" in err: print("‚ùå (429 –õ–∏–º–∏—Ç)")
            elif "404" in err: print("‚ùå (404 –ù–µ—Ç –¥–æ—Å—Ç—É–ø–∞)")
            else: print(f"‚ùå ({err})")
    
    print("üíÄ –í—Å–µ –º–æ–¥–µ–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã.")
    return False

# --- –û–ë–†–ê–ë–û–¢–ß–ò–ö–ò ---

@dp.message(CommandStart())
async def command_start_handler(message: Message):
    status = f"‚úÖ –ú–æ–¥–µ–ª—å: `{ACTIVE_MODEL_NAME}`" if ACTIVE_MODEL else "üíÄ –ù–µ—Ç —Å–≤—è–∑–∏ —Å AI"
    # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—è—Å–Ω–µ–Ω–∏–µ –ø—Ä–æ –ª–∏–º–∏—Ç—ã
    if "exp" in str(ACTIVE_MODEL_NAME):
        status += "\n(–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è - –ª–∏–º–∏—Ç—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –æ–∫)"
    elif "preview" in str(ACTIVE_MODEL_NAME):
        status += "\n‚ö†Ô∏è –í–Ω–∏–º–∞–Ω–∏–µ: Preview –≤–µ—Ä—Å–∏—è, –≤–æ–∑–º–æ–∂–µ–Ω –ª–∏–º–∏—Ç 20/–¥–µ–Ω—å."
        
    await message.answer(f"ü§ñ **Bot Reloaded**\n{status}")

@dp.message()
async def main_handler(message: Message):
    # Lazy loading: –µ—Å–ª–∏ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ –Ω–µ –≤—ã—à–ª–æ, –ø—Ä–æ–±—É–µ–º —Å–µ–π—á–∞—Å
    if not ACTIVE_MODEL:
        await message.answer("üîÑ –ò—â—É –∂–∏–≤—É—é –º–æ–¥–µ–ª—å...")
        if not await find_best_working_model():
            await message.answer("‚ùå –ë–µ–∑—É—Å–ø–µ—à–Ω–æ. Google –æ—Ç–∫–ª–æ–Ω–∏–ª –≤—Å–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã.")
            return

    bot_user = await bot.get_me()
    if not await is_addressed_to_bot(message, bot_user):
        return

    await bot.send_chat_action(chat_id=message.chat.id, action="typing")

    prompt_parts = [] 
    temp_files_to_delete = []

    try:
        text_content = ""
        if message.text:
            text_content = message.text.replace(f"@{bot_user.username}", "").strip()
        elif message.caption:
            text_content = message.caption.replace(f"@{bot_user.username}", "").strip()
        
        if text_content:
            prompt_parts.append(text_content)

        if message.photo:
            photo_id = message.photo[-1].file_id
            file_info = await bot.get_file(photo_id)
            img_data = BytesIO()
            await bot.download_file(file_info.file_path, img_data)
            img_data.seek(0)
            image = Image.open(img_data)
            prompt_parts.append(image)

        if message.voice:
            file_id = message.voice.file_id
            file_info = await bot.get_file(file_id)
            with tempfile.NamedTemporaryFile(suffix=".ogg", delete=False) as temp_audio:
                await bot.download_file(file_info.file_path, destination=temp_audio.name)
                temp_path = temp_audio.name
            temp_files_to_delete.append(temp_path)

            uploaded_file = genai.upload_file(path=temp_path, mime_type="audio/ogg")
            while uploaded_file.state.name == "PROCESSING":
                await asyncio.sleep(1)
                uploaded_file = genai.get_file(uploaded_file.name)

            prompt_parts.append(uploaded_file)
            prompt_parts.append("–ü–æ—Å–ª—É—à–∞–π –∏ –æ—Ç–≤–µ—Ç—å.")

        if not prompt_parts:
            await message.reply("–ü—É—Å—Ç–æ.")
            return

        response = await ACTIVE_MODEL.generate_content_async(prompt_parts)
        
        if response.text:
            await message.reply(response.text)
        else:
            await message.reply("...")

    except Exception as e:
        logging.error(f"Gen Error: {e}")
        # –ê–≤—Ç–æ-—Å–º–µ–Ω–∞ –º–æ–¥–µ–ª–∏ –ø—Ä–∏ –æ—à–∏–±–∫–µ
        if "429" in str(e) or "404" in str(e):
             await message.reply(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å {ACTIVE_MODEL_NAME} –∫–æ–Ω—á–∏–ª–∞—Å—å. –ò—â—É –¥—Ä—É–≥—É—é...")
             if await find_best_working_model():
                 # –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –ø–æ–≤—Ç–æ—Ä (–æ–¥–∏–Ω —Ä–∞–∑) –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å, –Ω–æ –ø–æ–∫–∞ –ø—Ä–æ—Å—Ç–æ —É–≤–µ–¥–æ–º–ª—è–µ–º
                 await message.reply(f"‚úÖ –ü–µ—Ä–µ—à–µ–ª –Ω–∞ {ACTIVE_MODEL_NAME}. –ü–æ–≤—Ç–æ—Ä–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ.")
             else:
                 await message.reply("üíÄ –ë–æ–ª—å—à–µ —Ä–∞–±–æ—á–∏—Ö –º–æ–¥–µ–ª–µ–π –Ω–µ—Ç.")
        else:
             await message.reply("–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏.")
    
    finally:
        for f_path in temp_files_to_delete:
            try:
                os.remove(f_path)
            except:
                pass

# --- SERVER ---

@app.get("/")
async def root():
    return {"status": "Alive", "model": ACTIVE_MODEL_NAME}

@app.get("/health")
async def health_check():
    return {"status": "ok"}

async def keep_alive_ping():
    if not RENDER_URL:
        return
    while True:
        await asyncio.sleep(300)
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(f"{RENDER_URL}/health") as resp:
                    logging.info(f"Ping: {resp.status}")
        except Exception:
            pass

async def start_bot():
    await find_best_working_model()
    await bot.delete_webhook(drop_pending_updates=True)
    await dp.start_polling(bot)

async def start_server():
    config = uvicorn.Config(app, host="0.0.0.0", port=10000, log_level="error")
    server = uvicorn.Server(config)
    await server.serve()

async def main():
    await asyncio.gather(start_server(), start_bot(), keep_alive_ping())

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        pass
